# Manufacturing Downtime Predictor

**Manufacturing Downtime Predictor** is a web-based application designed to predict machine downtime or production defects in a manufacturing environment. The application utilizes machine learning models trained on historical and real-time machine data to provide actionable insights, helping optimize production efficiency and reduce unplanned downtime.

## Objective
This project was developed to meet the following goals:

1. Build a predictive analysis model for manufacturing data.
2. Create RESTful API endpoints to enable uploading of data, model training, and prediction generation.
3. Provide an interactive front-end for users to interact with the application.

## Features
- **Data Upload**: Upload manufacturing data in CSV format.
- **Model Training**: Train machine learning models (Logistic Regression, Decision Tree, or Support Vector Machine) using uploaded data.
- **Downtime Prediction**: Predict machine downtime based on operational features and return predictions with confidence scores.
- **Synthetic Data Generation**: Generate manufacturing data with key fields like `Machine_ID`, `Temperature`, `Run_Time`, and `Downtime_Flag`.
- **RESTful API Endpoints**: Interact with the system programmatically to automate workflows.
- **Performance Metrics**: Provide detailed metrics such as accuracy, precision, recall, and F1-score after model training.
- **Download Synthetic Dataset**: Easily download generated synthetic datasets for testing and development purposes.
- **Interactive Front-End**: A user-friendly front-end built using HTML, CSS, and JavaScript for seamless user interaction.
- **Image Uploads for Documentation**: Users can upload images to visualize and document the application’s working interface.

## Key Technologies
- **Backend Framework**: Python Flask
- **Frontend Technologies**: HTML, CSS, JavaScript
- **Machine Learning Library**: scikit-learn
- **Data Handling**: pandas


## Setup Instructions

Follow these steps to set up and run the **Manufacturing Downtime Predictor** on your local machine.

### 1. Prerequisites
Before setting up the application, ensure that you have the following installed on your system:

- **Python 3.x**: The application is developed using Python.
- **Pip**: Python package manager to install the dependencies.
- **Git** (optional): For cloning the repository.

### 2. Clone the Repository
If you haven’t already, clone the repository containing the project files.

```bash
git clone https://github.com/your-username/manufacturing-downtime-predictor.git
cd manufacturing-downtime-predictor
```

### 3. Install Dependencies
Navigate to the project directory and install the required Python libraries using `pip`. The required dependencies are listed in the `requirements.txt` file.

Run the following command to install them:

```bash
pip install -r requirements.txt
```

This will install the following dependencies:
- `Flask`: Web framework for creating the application.
- `scikit-learn`: Machine learning library for training models.
- `pandas`: For data manipulation.
- `numpy`: For numerical operations.

### 4. Set Up the Flask Application
To run the application, you'll need to execute the following command.

  ```bash
    python app.py
  ```
This will start the server on `http://127.0.0.1:5000/`.

### 5. Access the Web Application
Once the server is running, open your browser and navigate to `http://127.0.0.1:5000/` to access the application’s user interface.

- You can upload your own data or use the generated synthetic dataset.
- Choose a machine learning model (Logistic Regression, Decision Tree, or SVM) and train the model.
- After training, make predictions based on new feature inputs.

### 6. Testing API Endpoints (Optional)
If you'd like to test the API endpoints directly (without the front-end interface), you can use tools like `curl` or Postman to make HTTP requests to the Flask server.

- **Upload Data**:
  ```bash
  curl -X POST -F "file=@data.csv" -F "target=Downtime_Flag" http://127.0.0.1:5000/upload
  ```

- **Train Model**:
  ```bash
  curl -X POST -H "Content-Type: application/json" -d '{"model_type": "dt"}' http://127.0.0.1:5000/train
  ```

- **Make Prediction**:
  ```bash
  curl -X POST -H "Content-Type: application/json" -d '{"features": {"Temperature": 85.0, "Run_Time": 350, "Torque": 45.5, "Tool_Wear": 150}}' http://127.0.0.1:5000/predict
  ```

### 7. Download Synthetic Data
To download the synthetic dataset generated by the application, use the `Generate Data` feature or use the following curl command:

```bash
curl -X POST http://127.0.0.1:5000/generate-data --output synthetic_data.csv
```

This will download the CSV file with the generated synthetic data.

---

## Files in the Project

### 1. `app.py`
This is the main Flask application that handles the server-side logic of the project. It serves the front-end, manages data uploads, model training, and prediction, and interacts with the machine learning models through API endpoints.

- **Key functionality**:
  - Routes and serves the web application using Flask.
  - Handles file uploads and dataset parsing.
  - Trains machine learning models using the uploaded dataset (`Logistic Regression`, `Decision Tree`, or `SVM`).
  - Makes predictions based on user input and the trained model.
  - Exposes RESTful APIs for uploading data, training models, generating predictions, and downloading synthetic data.

### 2. `model.py`
This file contains the `ModelTrainer` class, which is responsible for training the machine learning models and making predictions. It uses the `scikit-learn` library to build, train, and evaluate models like Logistic Regression, Decision Tree, and Support Vector Machine.

- **Key functionality**:
  - Handles the training of the machine learning models.
  - Prepares data for model training by splitting it into training and testing sets.
  - Trains models and evaluates them using metrics like accuracy, precision, recall, and F1-score.
  - Makes predictions based on input features.

### 3. `index.html`
This is the main front-end HTML file that provides the structure of the web interface. It allows users to upload datasets, train models, and make predictions. The file also includes buttons for navigating between different sections (uploading data, training models, and making predictions).

- **Key functionality**:
  - Defines the layout and structure of the web interface.
  - Includes forms for uploading data, selecting a machine learning model, and inputting feature values for predictions.
  - Dynamically generates the input fields for making predictions based on the dataset's features.

### 4. `script.js`
This file contains the JavaScript code that manages the dynamic interactions between the front-end and the Flask back-end. It updates the user interface in real-time based on user input and server responses, ensuring a smooth and responsive user experience.

- **Key functionality**:
  - Handles user interactions, including uploading data, selecting model types, and submitting prediction requests.
  - Dynamically generates input fields for making predictions based on the features of the uploaded dataset.
  - Sends asynchronous HTTP requests (using the `fetch` API) to the Flask back-end for uploading data, training machine learning models, and fetching predictions.
  - Updates the UI with success or error messages after processing data on the server.
  - Manages tab navigation and dynamically displays different sections of the user interface based on user selection.

### 5. `style.css`
This file contains the CSS styles for the web interface. It defines the visual appearance of the web application, including layouts, buttons, and form styles. It ensures that the application is user-friendly and visually appealing.

- **Key functionality**:
  - Styles the web page for a clean and professional look.
  - Defines the layout for different sections of the application (e.g., Upload, Train Model, Predict).
  - Provides styles for buttons, forms, and messages displayed on the web interface.

### 6. `data_gen.py`
This script is responsible for generating synthetic operational data for machines. It simulates various features such as temperature, run time, torque, and tool wear, and then assigns a downtime flag (`Downtime_Flag`) based on predefined conditions. The generated dataset is saved as a CSV file.

- **Key functionality**:
  - Generates machine data with features: `Machine_ID`, `Temperature`, `Run_Time`, `Torque`, and `Tool_Wear`.
  - Assigns the `Downtime_Flag` based on specific conditions (e.g., high temperature, long run time, high torque).
  - Saves the generated data to a CSV file (`machine_data.csv`).

### Synthetic Dataset
This CSV file contains the synthetic dataset generated by `data_gen.py`. It includes the following columns:
  - **Machine_ID**: Unique identifier for each machine.
  - **Temperature**: The operating temperature of the machine (°C).
  - **Run_Time**: Total run time of the machine (minutes).
  - **Torque**: The applied torque to the machine (Newton-meters).
  - **Tool_Wear**: The wear level of the machine's tool.
  - **Downtime_Flag**: A binary flag indicating if the machine is likely to experience downtime (`1` for downtime, `0` for no downtime).

---

## API Endpoints

### 1. `/upload` (POST)
- **Description**: Uploads a CSV file with manufacturing data and selects the target variable for prediction.
- **Input**: 
  - `file`: CSV file.
  - `target`: Target column (e.g., `Downtime_Flag`).
- **Output**: JSON with success message, feature names, and target variable.

### 2. `/train` (POST)
- **Description**: Trains the machine learning model using the uploaded dataset.
- **Input**: 
  - `model_type`: Model choice (`"lr"`, `"dt"`, `"svm"`).
- **Output**: JSON with success message and model performance metrics (accuracy, precision, recall, F1-score).

### 3. `/predict` (POST)
- **Description**: Accepts feature inputs and returns a downtime prediction along with confidence.
- **Input**: JSON with feature values (e.g., `Temperature`, `Run_Time`, `Torque`, `Tool_Wear`).
- **Output**: JSON with prediction (`"Yes"`/`"No"`) and confidence score.

### 4. `/generate-data` (POST)
- **Description**: Generates synthetic manufacturing data for testing.
- **Output**: CSV file for download.

--- 


## Usage Workflow
1. **Upload Data**: Navigate to the Upload tab, upload a dataset, and specify the target variable.
2. **Train the Model**: Select a machine learning algorithm and initiate model training. View performance metrics upon completion.
3. **Make Predictions**: Input feature values into the Predict tab to get a prediction for machine downtime.
4. **Download Synthetic Dataset**: Use the "Generate Data" feature to create and download synthetic data.
5. **Upload Images**: Upload images of the UI in action to document functionality.
   
## Dynamic Front-End Functionality
The front-end of the application dynamically updates the input fields for predictions based on the uploaded dataset and selected target variable. Using JavaScript:
- **Input Generation**: When a dataset is uploaded, the front-end retrieves the feature names via the `/upload` API.
- **Dynamic Forms**: HTML elements for each feature are generated dynamically, ensuring the user can input values for all required fields.
- **Target Selection**: The target variable is highlighted and removed from the input fields to avoid confusion.
- **Real-Time Updates**: The interface adapts automatically when new data is uploaded or the target is changed.

This dynamic functionality ensures a seamless user experience and reduces the potential for errors during prediction input.
## UI Screenshots
![image](https://github.com/user-attachments/assets/4aae5110-e956-4e03-966a-78058925d439)
![image](https://github.com/user-attachments/assets/73edbf1f-c4c0-48a5-a5f3-a379afdfbb36)
![image](https://github.com/user-attachments/assets/f9eb0d09-b618-408f-b018-a15d65305663)

## Available Models
The application supports three machine learning models, each chosen for its suitability to manufacturing data:

### Logistic Regression (LR)
- **Rationale**: Logistic Regression is effective for linearly separable data and provides probabilistic predictions. It is used here for simplicity and interpretability.
- **Performance**: Achieved 95.50% accuracy on synthetic data, making it a strong baseline for linearly dependent datasets.

### Decision Tree (DT)
- **Rationale**: Decision Trees naturally handle non-linear relationships and interactions between features. For this dataset, which shows non-linear dependency on temperature and run-time, the Decision Tree achieved high accuracy.
- **Performance**: Achieved perfect metrics (100% for accuracy, precision, recall, and F1 score) on the synthetic dataset.

### Support Vector Machine (SVM)
- **Rationale**: SVM is suited for high-dimensional and complex decision boundaries. By using a non-linear kernel, it effectively separates challenging datasets.
- **Performance**: Achieved 97.75% accuracy, with precision of 92.31%, recall of 60.00%, and F1-score of 72.73% on the synthetic dataset.

##
## Contributors
- [Raghav Arora](https://github.com/RaghavArora14)

## License
This project is licensed under the Apache License 2.0.

